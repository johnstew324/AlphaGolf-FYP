# robust_dataset_builder.py
import os
import sys
import pandas as pd
import numpy as np
import json
from datetime import datetime
import logging
import traceback

# Add project root to path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# Import components
from database import DatabaseManager
from config import Config
from data_Excator.data_excractor import DataExtractor
from feature_engineering.processors.course_stats_processor import CourseStatsProcessor
from feature_engineering.processors.tournament_weather_processor import TournamentWeatherProcessor
from feature_engineering.processors.scorecard_processor import ScorecardProcessor
from feature_engineering.processors.current_form_processor import CurrentFormProcessor
from feature_engineering.processors.course_fit_processor import CourseFitProcessor
from feature_engineering.processors.tournament_history_processor import TournamentHistoryProcessor
from feature_engineering.processors.tournament_history_stats_processor import TournamentHistoryStatsProcessor
from feature_engineering.processors.player_stats_processor import PlayerFormProcessor
from feature_engineering.processors.player_profile_overview_processor import PlayerProfileProcessor
from feature_engineering.processors.player_career_processor import PlayerCareerProcessor

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dataset_builder.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def process_tournament_features(tournament_id, season, processors, selected_player_ids=None):
    """
    Process a single tournament's features using all provided processors.
    
    Args:
        tournament_id: Tournament ID to process
        season: Season year
        processors: Dict of processor instances
        selected_player_ids: Optional list of player IDs to filter by
        
    Returns:
        DataFrame with combined features for this tournament
    """
    logger.info(f"Processing features for tournament {tournament_id} (Season {season})...")
    
    # Store results from each processor
    processor_results = {}
    
    # Process each processor
    for name, processor in processors.items():
        try:
            logger.info(f"Running {name} processor...")
            start_time = datetime.now()
            
            # Extract features
            features = processor.extract_features(
                tournament_id=tournament_id, 
                player_ids=selected_player_ids, 
                season=season
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            if features is None or features.empty:
                logger.warning(f"No features generated by {name} processor")
                continue
                
            logger.info(f"Successfully generated {len(features)} records with {features.shape[1]} features")
            logger.info(f"Processing time: {elapsed:.2f} seconds")
            
            # Store results
            processor_results[name] = features
            
        except Exception as e:
            logger.error(f"Error in {name} processor: {str(e)}")
            logger.error(traceback.format_exc())
    
    # Check if we have any successful results
    successful_dfs = [df for df in processor_results.values() if df is not None and not df.empty]
    if not successful_dfs:
        logger.warning(f"No valid features generated for tournament {tournament_id}")
        return None
    
    # Combine all successful feature sets
    logger.info(f"Combining {len(successful_dfs)} feature sets...")
    try:
        # Start with the first DataFrame
        combined = successful_dfs[0].copy()
        
        # Ensure tournament_id is present
        if 'tournament_id' not in combined.columns:
            combined['tournament_id'] = tournament_id
        
        # Merge with other DataFrames
        for i, df in enumerate(successful_dfs[1:], 1):
            logger.info(f"Merging feature set {i}...")
            
            # Skip if this DataFrame doesn't have player_id
            if 'player_id' not in df.columns:
                logger.warning(f"Feature set {i} doesn't have player_id column, adding tournament ID column only")
                continue
                
            if 'player_id' not in combined.columns:
                logger.warning("Combined DataFrame doesn't have player_id column yet, copying from current DataFrame")
                combined['player_id'] = df['player_id']
                # Add other columns
                for col in df.columns:
                    if col != 'player_id' and col not in combined.columns:
                        combined[col] = df[col]
                continue
            
            # Ensure tournament_id is present in this DataFrame
            if 'tournament_id' not in df.columns:
                df['tournament_id'] = tournament_id
            
            # Merge with combined DataFrame
            combined = pd.merge(
                combined, df, 
                on=['player_id'], 
                how='outer',
                suffixes=('', f'_{i}')
            )
            
            # Handle duplicate columns
            duplicate_cols = [c for c in combined.columns if c.endswith(f'_{i}')]
            for dup_col in duplicate_cols:
                original_col = dup_col.replace(f'_{i}', '')
                
                # If original column exists, keep it; otherwise, rename duplicate
                if original_col in combined.columns:
                    combined.drop(columns=[dup_col], inplace=True)
                else:
                    combined.rename(columns={dup_col: original_col}, inplace=True)
        
        logger.info(f"Successfully combined features - final shape: {combined.shape}")
        return combined
        
    except Exception as e:
        logger.error(f"Error combining features: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def process_tournament_targets(tournament_id, season, data_extractor):
    """
    Generate target variables for a tournament.
    
    Args:
        tournament_id: Tournament ID
        season: Season year
        data_extractor: DataExtractor instance
        
    Returns:
        DataFrame with target variables
    """
    logger.info(f"Generating target variables for tournament {tournament_id}...")
    
    try:
        # Convert to special format for tournament_history if needed
        special_tournament_id = tournament_id
        if tournament_id.startswith("R") and len(tournament_id) >= 8:
            special_tournament_id = "R2025" + tournament_id[5:]
            logger.info(f"Using special tournament ID format: {special_tournament_id} for history lookup")
        
        # Extract tournament results
        history = data_extractor.extract_tournament_history(
            tournament_ids=special_tournament_id
        )
        
        if history.empty:
            logger.warning(f"No tournament history found for {tournament_id}")
            return None
            
        logger.info(f"Found {len(history)} tournament history records")
        
        # Check for required columns
        required_columns = ['position_numeric', 'player_id']
        missing_columns = [col for col in required_columns if col not in history.columns]
        
        if missing_columns:
            logger.error(f"Missing required columns in tournament history: {missing_columns}")
            logger.info(f"Available columns: {history.columns.tolist()}")
            
            # Try to fix common issues
            if 'player_id' not in history.columns and 'PlayerID' in history.columns:
                history['player_id'] = history['PlayerID']
                logger.info("Mapped PlayerID to player_id")
            
            # Check again after fixes
            missing_columns = [col for col in required_columns if col not in history.columns]
            if missing_columns:
                logger.error("Still missing required columns after attempted fixes")
                return None
        
        # Create target variables
        targets = []
        
        for _, player_data in history.iterrows():
            try:
                player_target = {
                    'player_id': player_data['player_id'],
                    'tournament_id': tournament_id,
                    'year': season
                }
                
                # Position-based targets
                if 'position_numeric' in player_data and pd.notna(player_data['position_numeric']):
                    pos = player_data['position_numeric']
                    player_target['position'] = pos
                    player_target['winner'] = 1 if pos == 1 else 0
                    player_target['top3'] = 1 if pos <= 3 else 0
                    player_target['top10'] = 1 if pos <= 10 else 0
                    player_target['made_cut'] = 1 if pos < 100 else 0  # Assuming position >= 100 means missed cut
                
                targets.append(player_target)
            except Exception as e:
                logger.error(f"Error processing target for player: {str(e)}")
                continue
        
        # Create DataFrame
        if not targets:
            logger.warning("No valid targets created")
            return None
            
        targets_df = pd.DataFrame(targets)
        
        # Ensure proper types
        for col in ['winner', 'top3', 'top10', 'made_cut']:
            if col in targets_df.columns:
                targets_df[col] = targets_df[col].astype(int)
        
        logger.info(f"Generated {len(targets_df)} target records")
        return targets_df
        
    except Exception as e:
        logger.error(f"Error generating targets: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def build_dataset(tournament_ids, output_dir='data', feature_sets_path=None):
    """
    Build a dataset for golf tournament prediction.
    
    Args:
        tournament_ids: List of tournament IDs to include
        output_dir: Directory to save the dataset
        feature_sets_path: Optional path to optimized feature sets JSON
        
    Returns:
        Dict with paths to created datasets
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Load optimized feature sets if provided
    optimized_features = {}
    if feature_sets_path:
        try:
            logger.info(f"Loading optimized feature sets from {feature_sets_path}")
            with open(feature_sets_path, 'r') as f:
                feature_sets = json.load(f)
                
            # Extract optimized sets
            if 'optimized_sets' in feature_sets:
                optimized_features = feature_sets['optimized_sets']
                logger.info(f"Loaded optimized feature sets for {len(optimized_features)} targets")
        except Exception as e:
            logger.error(f"Error loading feature sets: {str(e)}")
            # Continue without optimized features
    
    # Initialize database connection
    logger.info("Connecting to MongoDB...")
    db_manager = DatabaseManager(uri=Config.MONGODB_URI, database_name="pga_tour_data")
    logger.info("Successfully connected to database")
    
    # Create data extractor
    extractor = DataExtractor(db_manager)
    
    # Create all processors
    processors = {
        "Player Form": PlayerFormProcessor(extractor),
        "Course Fit": CourseFitProcessor(extractor),
        "Tournament History": TournamentHistoryProcessor(extractor),
        "Player Profile": PlayerProfileProcessor(extractor),
        "Player Career": PlayerCareerProcessor(extractor),
        "Scorecard": ScorecardProcessor(extractor),
        "Course Stats": CourseStatsProcessor(extractor),
        "Current Form": CurrentFormProcessor(extractor),
        "Tournament History Stats": TournamentHistoryStatsProcessor(extractor),
        "Tournament Weather": TournamentWeatherProcessor(extractor)
    }
    
    # Process each tournament
    all_features = []
    all_targets = []
    
    for tournament_id in tournament_ids:
        # Extract the season from tournament ID
        season = int(tournament_id[1:5]) if tournament_id.startswith("R") and len(tournament_id) >= 8 else 2024
        
        # Process tournament features
        tournament_features = process_tournament_features(tournament_id, season, processors)
        
        if tournament_features is None:
            logger.warning(f"Skipping tournament {tournament_id} - no valid features")
            continue
            
        # Generate target variables
        tournament_targets = process_tournament_targets(tournament_id, season, extractor)
        
        if tournament_targets is None:
            logger.warning(f"No target variables for tournament {tournament_id} - skipping")
            continue
            
        # Store tournament data
        all_features.append(tournament_features)
        all_targets.append(tournament_targets)
    
    # Check if we have any valid data
    if not all_features or not all_targets:
        logger.error("No valid tournament data collected")
        return None
        
    # Combine all tournaments
    logger.info("Combining all tournament data...")
    try:
        features_df = pd.concat(all_features, ignore_index=True)
        targets_df = pd.concat(all_targets, ignore_index=True)
        
        logger.info(f"Combined features shape: {features_df.shape}")
        logger.info(f"Combined targets shape: {targets_df.shape}")
        
        # Merge features and targets
        logger.info("Creating final dataset by merging features and targets...")
        master_df = pd.merge(
            features_df,
            targets_df,
            on=['player_id', 'tournament_id'],
            how='inner'
        )
        
        logger.info(f"Master dataset shape: {master_df.shape}")
        
        # Save master dataset
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        master_path = os.path.join(output_dir, f'master_dataset_{timestamp}.csv')
        
        logger.info(f"Saving master dataset to {master_path}")
        master_df.to_csv(master_path, index=False)
        
        # Create train/test split
        tournaments_df = pd.DataFrame({
            'tournament_id': tournament_ids,
            'season': [int(tid[1:5]) if tid.startswith("R") and len(tid) >= 8 else 0 for tid in tournament_ids]
        })
        
        # Sort by season
        tournaments_df = tournaments_df.sort_values('season')
        
        # Standard 80/20 chronological split
        train_size = int(len(tournaments_df) * 0.8)
        
        train_tournaments = tournaments_df.iloc[:train_size]['tournament_id'].tolist()
        test_tournaments = tournaments_df.iloc[train_size:]['tournament_id'].tolist()
        
        split_info = {
            'train_tournaments': train_tournaments,
            'test_tournaments': test_tournaments,
            'timestamp': timestamp
        }
        
        # Save split info
        split_path = os.path.join(output_dir, f'train_test_split_{timestamp}.json')
        with open(split_path, 'w') as f:
            json.dump(split_info, f, indent=2)
            
        # Create train/test datasets
        train_df = master_df[master_df['tournament_id'].isin(train_tournaments)]
        test_df = master_df[master_df['tournament_id'].isin(test_tournaments)]
        
        train_path = os.path.join(output_dir, f'train_dataset_{timestamp}.csv')
        test_path = os.path.join(output_dir, f'test_dataset_{timestamp}.csv')
        
        train_df.to_csv(train_path, index=False)
        test_df.to_csv(test_path, index=False)
        
        logger.info(f"Created train dataset with {len(train_df)} rows")
        logger.info(f"Created test dataset with {len(test_df)} rows")
        
        # Create target-specific datasets if optimized features are available
        target_paths = {}
        
        if optimized_features:
            for target_type, features in optimized_features.items():
                try:
                    # Map target type to column name
                    target_map = {
                        'win': 'winner',
                        'cut': 'made_cut',
                        'top3': 'top3',
                        'top10': 'top10'
                    }
                    
                    target_col = target_map.get(target_type, target_type)
                    
                    if target_col not in master_df.columns:
                        logger.warning(f"Target column '{target_col}' not found in master dataset")
                        continue
                        
                    # Filter features that exist in dataset
                    valid_features = [f for f in features if f in master_df.columns]
                    
                    # Add essential columns
                    for col in ['player_id', 'tournament_id', target_col]:
                        if col not in valid_features:
                            valid_features.append(col)
                    
                    # Create target dataset
                    target_df = master_df[valid_features]
                    target_path = os.path.join(output_dir, f'{target_type}_dataset_{timestamp}.csv')
                    
                    target_df.to_csv(target_path, index=False)
                    logger.info(f"Created {target_type} dataset with {len(target_df)} rows and {len(valid_features)} features")
                    
                    target_paths[target_type] = target_path
                    
                except Exception as e:
                    logger.error(f"Error creating {target_type} dataset: {str(e)}")
        
        # Return all dataset paths
        return {
            'master': master_path,
            'train': train_path,
            'test': test_path,
            'split': split_path,
            'targets': target_paths
        }
        
    except Exception as e:
        logger.error(f"Error creating final dataset: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def analyze_dataset(dataset_path):
    """
    Analyze a dataset and print statistics.
    
    Args:
        dataset_path: Path to dataset CSV
        
    Returns:
        Dict with statistics
    """
    logger.info(f"Analyzing dataset: {dataset_path}")
    
    try:
        # Load dataset
        df = pd.read_csv(dataset_path)
        
        # Basic info
        rows = len(df)
        cols = len(df.columns)
        
        logger.info(f"Dataset shape: {rows} rows, {cols} columns")
        
        # Tournament and player counts
        tournament_count = df['tournament_id'].nunique() if 'tournament_id' in df.columns else 0
        player_count = df['player_id'].nunique() if 'player_id' in df.columns else 0
        
        logger.info(f"Contains {tournament_count} tournaments and {player_count} players")
        
        # Target variables stats
        target_stats = {}
        for target in ['winner', 'top3', 'top10', 'made_cut']:
            if target in df.columns:
                positives = df[target].sum()
                percentage = (positives / rows) * 100
                
                logger.info(f"{target}: {positives} positives ({percentage:.2f}%)")
                
                target_stats[target] = {
                    'positive_count': int(positives),
                    'total_count': rows,
                    'percentage': float(percentage)
                }
        
        # Missing values
        missing_counts = df.isna().sum()
        missing_pct = (missing_counts / rows) * 100
        high_missing = missing_pct[missing_pct > 25].sort_values(ascending=False)
        
        if not high_missing.empty:
            logger.info("Features with high missing values (>25%):")
            for feature, pct in high_missing.items():
                logger.info(f"  {feature}: {pct:.2f}%")
        
        # Return statistics
        return {
            'rows': rows,
            'columns': cols,
            'tournaments': tournament_count,
            'players': player_count,
            'target_stats': target_stats,
            'high_missing': high_missing.to_dict()
        }
        
    except Exception as e:
        logger.error(f"Error analyzing dataset: {str(e)}")
        return None

if __name__ == "__main__":
    # Define tournament IDs to include in dataset
    tournament_ids = [
        # 2022 Tournaments
        "R2022010", "R2022018", "R2022030", "R2022033", "R2022041",
        
        # 2023 Tournaments
        "R2023011", "R2023019", "R2023026", "R2023035", "R2023047",
        
        # 2024 Tournaments
        "R2024005", "R2024012", "R2024016", "R2024021", "R2024028",
        
        # 2025 Early Tournaments (for testing)
        "R2025002", "R2025004"
    ]
    
    # Path to optimized feature sets (if available)
    feature_sets_path = 'feature_refinement/optimized_feature_sets.json'
    
    # Build the dataset
    logger.info("Starting dataset build process...")
    dataset_paths = build_dataset(tournament_ids, feature_sets_path=feature_sets_path)
    
    if dataset_paths:
        logger.info("Dataset build successful!")
        
        # Analyze the master dataset
        analyze_dataset(dataset_paths['master'])
        
        # Print paths to created files
        logger.info("\nCreated dataset files:")
        for key, path in dataset_paths.items():
            if key != 'targets':
                logger.info(f"- {key}: {path}")
        
        if 'targets' in dataset_paths:
            logger.info("\nTarget-specific datasets:")
            for target, path in dataset_paths['targets'].items():
                logger.info(f"- {target}: {path}")
    else:
        logger.error("Dataset build failed")